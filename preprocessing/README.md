# Multimodal Feature Extraction

reCAPit enables the analysis of various derived features (**artifacts**) extracted from multiple data sources, including video, speech, and eye tracking (**sources**).
All **sources** and **artifacts** are managed using a JSON file called `manifest.json`, which contains both manually defined and automatically generated entries.
The provided scripts automatically add `artifact` entries (**registering**) to the `manifest.json` file based on the **sources** you manually specify.

A basic `manifest.json` file has the following structure:

```
{
    "language": "english",
    "duration_sec": 4915.0,
    "roles": [],
    "recordings": [],
    "sources": {},
    "artifacts": {}
}
```

## Table of Contents

- [Sources](#sources)
- [Artifacts](#artifacts)
- [Running Scripts](#Running-Scripts)
- [📄 Transcript](#-transcript)
  - [`register_transcript.py`](#register_transcriptpy)
- [🧩 Segmentation](#-segmentation)
  - [`register_segment_initial.py`](#register_segment_initialpy)
  - [`register_segment_refine.py`](#register_segment_refinepy)
  - [`segment_attributes.py`](#segment_attributespy)
- [🎥 Video](#-video)
  - [`register_movement.py`](#register_movementpy)
  - [`register_heatmaps_gaze.py`](#register_heatmaps_gazepy)
  - [`register_heatmaps_move.py`](#register_heatmaps_movepy)
- [👁️ Gaze](#️-gaze)
  - [`register_attention.py`](#register_attentionpy)
- [🗒️ Digital Notes](#️-digital-notes)
  - [`register_notes.py`](#register_notespy)

## Sources

A source can be specified in the `manifest.json` file either as global or specific to individual recordings.
Each source must be specified with a `path` (an absolute path to the file) and `offset_sec` (which can be used to synchronize different sources). This offset is typically necessary unless all sources start recording at the same time. A complete example `manifest.json` file can be found in this repository.

The following sources are currently supported:

* `videos/workspace` - A video recording of the shared workspace area, ideally from a top-down perspective.
* `videos/room` - A video recording of the room or external perspective.
* `audio` - An audio recording file containing speech data for transcript generation.
* `areas_of_interests` - A JSON file generated by [labelme](https://github.com/wkentaro/labelme) that partitions the working area into semantically meaningful areas with assigned labels.
* `notes_snapshots` - A directory containing timestamped Word document snapshots (.docm files) for analyzing temporal changes in digital notes.
* `surface_fixations` - Eye tracking files containing fixations with (normalized) coordinates mapped to the working area.

The following snippet shows how to specify `surface_fixations` in the `manifest.json`:

```
"recordings": [
    {
        "id": "Carl",
        "role": "domain",
        "sources": {
            "surface_fixations": {
                "path": "Carl/surface_fix.csv",
                "offset_sec": 367.0
            }
        }
    },
]
```

The following snippet shows how to specify `areas_of_interests` and `videos` in the `manifest.json`:

```
"sources": {
    "areas_of_interests": {
        "path": "aois_labelme.json",
        "offset_sec": 0.0
    },
    "videos": {
        "workspace": {
            "path": "workspace.mp4",
            "offset_sec": 312.27
        },
        "room": {
            "path": "room.mp4",
            "offset_sec": 312.27
        }
    }
}
```
## Artifacts

The scripts provided in this folder can be used to **register** a variety of **artifacts** based on the specified sources.
It should be noted that only artifacts registered in the `manifest.json` can be used by the reCAPit frontend.
Artifacts are be either recording specific, i.e. are linked to one participant, or global, meaning they describe aggregated data covering more than one participant. 

### Global Artifacts

The following global artifacts can be generated by the preprocessing scripts and registered in the manifest:

* `artifacts/transcript` - Processed transcript data with speaker mapping.
* `artifacts/segments/initial` - Initial segmentation results based on multivariate time series data.
* `artifacts/segments/refined` - Refined segmentation results using lexical features from transcripts.
* `artifacts/multi_time/movement` - Multivariate time series data representing movement activity extracted from workspace video.
* `artifacts/multi_time/attention` - Multivariate time series data representing attention signals derived from eye tracking data.
* `artifacts/video_overlay/attention` - Gaze-based heatmap overlays on workspace video showing attention patterns.
* `artifacts/video_overlay/movement` - Movement-based heatmap overlays showing hand activity patterns in areas of interest.
* `artifacts/notes` - Temporal analysis of digital note changes with diff visualizations.

### Recording Artifacts

The following recording artifacts can be generated by the preprocessing scripts and registered in the manifest:

* `artifacts/transcript` - Processed transcript data with speaker mapping 
* `artifacts/mapped_fixations` - Eye tracking fixations mapped to areas of interest

## Running Scripts

Scripts are organized in directories according to the sources they extract features from (transcript/video/gaze).
Each script that can register artifacts follows this naming convention: `register_*.py`.
Please ensure that [astral-sh/uv](https://github.com/astral-sh/uv) is installed on your system.
UV is a Python package and project manager that simplifies the deployment of Python applications.
To launch a script, navigate to the respective directory and run `uv run register_[NAME].py` in your terminal, replacing `NAME` with the respective script name.

> [!NOTE]
> Some scripts may also depend on artifacts produced by other scripts.

> [!NOTE]
> Please note that certain scripts depend on PyTorch. For best performance, it is recommended to run them on systems with a GPU and CUDA capabilities. Please make sure to install a PyTorch version that is compatible with the CUDA version available on your system.

## 📄 Transcript

### `register_transcript.py`

Registers transcript data and processes it for individual recordings by mapping speaker information to recording IDs and roles.

* 📥 This script requires a transcript file (CSV format) and recordings defined in the manifest
* 📤 This script will register `artifacts/transcript` globally and `artifacts/transcript` for each individual recording.

Please note that the actual transcript generation must be performed with an external tool.
There are many open-source tools that can be used for speech-to-text transformation, such as:

- **noScribe - https://github.com/kaixxx/noScribe** 
- **whisper-standalone-win - https://github.com/Purfview/whisper-standalone-win**

The transcript must be provided as a CSV file with the following required columns:

| Column | Type | Description |
|--------|------|-------------|
| `speaker` | string | Speaker identifier that matches recording IDs in the manifest |
| `text` | string | Transcribed speech content |
| `start timestamp [sec]` | float | Start time of the speech segment in seconds |
| `end timestamp [sec]` | float | End time of the speech segment in seconds |


> [!IMPORTANT]
> The speaker IDs must exactly match the recording IDs defined in the manifest. Many tools struggle with precise speaker identification and will most likely produce suboptimal results with more than two speakers. Hence, often manual corrections of the speaker assignments are necessary.

> [!TIP]
> Many tools such as the ones mentioned above output subtitle files (.srt or .vtt), which you can transform to the required CSV format using the helper scripts `transcript/srt2csv.py` and `transcript/vtt2csv.py`.


## 🧩 Segmentation

### `register_segment_initial.py` 

Performs initial segmentation based on a previously registered multivariate time series, specified by `input_signal`.

* 📥 The `input_signal` must be a registered multivariate time series `artifacts/multi_time`
* 📤 This script will register `artifacts/segments/initial`.

> [!NOTE]
> Currently, `movement` and `attention` are available, which you can extract using the scripts `videos/workspace/register_movement.py` and `gaze/register_attention.py`, respectively.

### `register_segment_refine.py` 

Refines an initial segmentation using lexical features extracted from the transcript. This refinement effectively splits the initial segments by detecting transitions between subsequent discussions.

* 📥 This script requires a previously registered initial segmentation `artifacts/segments/initial` (see [register_segment_initial.py](#register_segment_initialpy)) and a registered transcript `artifacts/transcript`
* 📤 This script will register `artifacts/segments/refined`.

> [!NOTE]
> Refinement is not a necessary step, and it is legitimate to perform only the initial segmentation. However, if you notice particularly long segments after initial segmentation, you may be advised to perform refinement.

### `segment_attributes.py` 

Uses ChatGPT to automatically generate text summaries and titles for each segment of an existing segmentation result.
The outputs are populated into the existing segmentation results as new data columns.

* 📥 This script requires a previously registered segmentation, either `artifacts/segments/initial` or `artifacts/segments/refined`
* 📤 This script will not register anything

> [!NOTE]
> This is a special script that does not register any new artifact, but instead adds new data to an existing segmentation result.

## 🎥 Video

### `register_movement.py`

Extracts movement activity from workspace video using background subtraction and hand detection. This script analyzes video frames to detect hand movements within defined areas of interest.

* 📥 This script requires a registered workspace video `sources/videos/workspace` and areas of interest `sources/areas_of_interests`
* 📤 This script will register a multivariate time series `artifacts/multi_time/movement`.

> [!NOTE]
> The output of this script `movement` can be used as an input signal for [register_segment_initial.py](#register_segment_initialpy)

### `register_heatmaps_gaze.py`

Generates gaze-based heatmaps from eye tracking data by creating temporal aggregations of fixation data overlaid on the workspace video.

* 📥 This script requires a registered workspace video `sources/videos/workspace` and mapped fixations from recordings with `artifacts/mapped_fixations` (see [register_attention.py](#register_attentionpy))
* 📤 This script will register `artifacts/video_overlay/attention`.

### `register_heatmaps_move.py`

Creates movement-based heatmaps by analyzing hand activity patterns within areas of interest over time windows.

* 📥 This script requires a registered workspace video `sources/videos/workspace` and areas of interest `sources/areas_of_interests` as a source.
* 📤 This script will register `artifacts/video_overlay/movement`.

## 👁️ Gaze

### `register_attention.py`

Processes eye tracking data to compute attention signals by mapping surface fixations to areas of interest and generating time series data.

* 📥 This script requires recordings with `sources/surface_fixations` and areas of interest `sources/areas_of_interests`
* 📤 This script will register a global artifact `artifacts/multi_time/attention` and recording specific artifacts `artifacts/mapped_fixations`.

The surface fixations must be provided as a CSV file with the following required columns:

| Column | Type | Description |
|--------|------|-------------|
| `mapped y [px]` | float | Normalized X coordinate on the working area (between 0 and 1). |
| `mapped_x [px]` | float | Normalized Y coordinate on the working area (between 0 and 1) |
| `within_surface` | bool | Flag indicating if the fixation is inside the working area |
| `start timestamp [sec]` | float | Start time of the eye fixation in seconds |
| `end timestamp [sec]` | float | End time of the eye fixation in seconds |

> [!NOTE]
> The global artifact of this script `attention` can be used as an input signal for [register_segment_initial.py](#register_segment_initialpy)

## 🗒️ Digital Notes

### `register_notes.py`

Analyzes temporal changes in an instrumented Word document by processing document snapshots and computing text differences between versions. The instrumented Word document `notes/notes_template.docm` can be used to record notes *during* the recording. It contains macros that periodically write snapshots of the current document state to a directory. You can use this document like any normal Word document, but you must enable macro execution when prompted by Word.

* 📥 This script requires notes snapshots `sources/notes_snapshots` generated by the instrumented Word document `notes/notes_template.docm` (directory containing .docm files with timestamp-based filenames).
* 📤 This script will register `artifacts/notes`.

> [!NOTE]
> The script uses diff-match-patch algorithms to identify insertions and deletions between document versions. It samples documents at configurable intervals (default: 150 seconds) and generates HTML visualizations of changes. Document filenames should be timestamps for proper temporal ordering.
